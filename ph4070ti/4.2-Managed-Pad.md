# Parallle, managed, padding
## Code
```c
52: void pad_input(float* restrict X, Conv_Layer* l) {
#pragma acc parallel loop
  for ( int c = 0; c < l->in_depth; c++) {
    #pragma acc loop
    for (int j = 0; j < l->in_height; j++) {
      #pragma acc loop
      for (int i = 0; i < l->in_width; i++) {
        int padded_idx = (j + l->padding) * l->padded_width + (i + l->padding) + c * l->padded_height * l->padded_width;
        int in_idx = j * l->in_width + i + c * l->in_height * l->in_width;
        l->in_padded[padded_idx] = X[in_idx];
      }
    }
  }
}


70: void conv_forward(float* restrict X, Conv_Layer* l, float* restrict Y) {
  
  pad_input(X, l); //Create input with zero-padding
   // For each output feature map
#pragma acc parallel loop  
    for ( int m = 0; m < l->out_depth; m++) {
      #pragma acc loop
      for (int j = 0; j < l->out_height; j++) {
       #pragma acc loop
        for (int i = 0; i < l->out_width; i++) {
          int y_idx = i + (l->out_width * (j + m * l->out_height)); 
          // Calculate dot product of Weights*Input
          float sum = 0.0f;
        #pragma acc loop  reduction(+:sum) 
          for (int c = 0; c < l->in_depth; c++) {
            for (int f_j = 0; f_j < l->filter_width; f_j++) {
              for (int f_i = 0; f_i < l->filter_width; f_i++) {
                int f_idx = f_i + (f_j * l->filter_width) + (c + m * l->in_depth) * (l->filter_width * l->filter_width); 
                int x_j = j * l->stride + f_j; 
                int x_i = i * l->stride + f_i; 
                int x_idx = c * l->padded_height * l->padded_width + x_j * l->padded_width + x_i; 
                sum += l->weights[f_idx] * l->in_padded[x_idx];
              } // for f_i
```

## Minfo

```
$ make
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c main.c -o main.o
arr2txt:
    332, Zero trip check eliminated
arr2txt_2:
    354, Zero trip check eliminated
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c layers.c -o layers.o
pad_input:
     52, Generating NVIDIA GPU code
         54, #pragma acc loop gang /* blockIdx.x */
         56, #pragma acc loop seq
         58, #pragma acc loop vector(128) /* threadIdx.x */
     52, Generating implicit copy(l) [if not already present]
         Generating implicit copyin(X[:]) [if not already present]
     56, Loop is parallelizable
     58, Loop is parallelizable
conv_forward:
     72, Generating NVIDIA GPU code
         75, #pragma acc loop gang /* blockIdx.x */
         77, #pragma acc loop seq
         79, #pragma acc loop seq
         84, #pragma acc loop seq
         85, #pragma acc loop seq
         86, #pragma acc loop vector(128) /* threadIdx.x */
             Generating reduction(+:sum)
     72, Generating implicit copyin(l) [if not already present]
         Generating implicit copy(Y[:]) [if not already present]
     77, Loop is parallelizable
     79, Loop is parallelizable
     84, Loop is parallelizable
     85, Loop is parallelizable
     86, Loop is parallelizable
     91, FMA (fused multiply-add) instruction(s) generated
pool_forward:
    173, Zero trip check eliminated
fc_forward:
    228, FMA (fused multiply-add) instruction(s) generated
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c malloc2D.c -o malloc2D.o
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c timer.c -o timer.o
cpu_timer_stop:
     11, FMA (fused multiply-add) instruction(s) generated
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -o cnn-cifar10 main.o layers.o malloc2D.o timer.o
```

## Execution
```
$ ./cnn-cifar10 
Parallel (pad) Code
CNN for 50000 images
Loading input batch 1...
Loading input batch 2...
Loading input batch 3...
Loading input batch 4...
Loading input batch 5...
Load Data time:0.279520 seconds
Create Network time:0.000708 seconds
Load Network Parameters time:0.003379 seconds
Create Ouputs time:0.000271 seconds

Net Forward total time:125.458308 seconds
    Time for conv1: 51.630514 seconds
    Time for relu1: 11.338526 seconds
    Time for pool1: 1.606187 seconds
    Time for conv2: 33.939334 seconds
    Time for relu2: 7.685577 seconds
    Time for pool2: 1.720474 seconds
    Time for conv3: 12.358397 seconds
    Time for relu3: 4.330365 seconds
    Time for pool3: 0.698481 seconds
    Time for fc: 0.131301 seconds
    Time for softmax: 0.006610 seconds

  Conv: 97.928245 seconds
  ReLU: 23.354468 seconds
  Pool: 4.025142 seconds
  FC:   0.131301 seconds
  Softmax: 0.006610 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000272 seconds
Free memory time:0.016246 seconds
Total time:125.758705 seconds
END!
```

## Profiling
### nv_time
```
$ ./cnn-cifar10 
Parallel (pad) Code
CNN for 50000 images
Loading input batch 1...
Loading input batch 2...
Loading input batch 3...
Loading input batch 4...
Loading input batch 5...
Load Data time:0.277006 seconds
Create Network time:0.000730 seconds
Load Network Parameters time:0.003374 seconds
Create Ouputs time:0.000270 seconds

Net Forward total time:128.574797 seconds
    Time for conv1: 52.655505 seconds
    Time for relu1: 11.399266 seconds
    Time for pool1: 1.606937 seconds
    Time for conv2: 34.924510 seconds
    Time for relu2: 7.723273 seconds
    Time for pool2: 1.720945 seconds
    Time for conv3: 13.310441 seconds
    Time for relu3: 4.382692 seconds
    Time for pool3: 0.699157 seconds
    Time for fc: 0.131266 seconds
    Time for softmax: 0.007934 seconds

  Conv: 100.890456 seconds
  ReLU: 23.505231 seconds
  Pool: 4.027039 seconds
  FC:   0.131266 seconds
  Softmax: 0.007934 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000282 seconds
Free memory time:0.016495 seconds
Total time:128.872954 seconds
END!

Accelerator Kernel Timing data
/home/gskondras/cifar/cnn-cifar10_0/1.3-Parallel-pad/layers.c
  pad_input  NVIDIA  devicenum=0
    time(us): 9,456,604
    52: compute region reached 150000 times
        52: kernel launched 150000 times
            grid: [1024]  block: [128]
             device time(us): total=9,456,604 max=286 min=45 avg=63
            elapsed time(us): total=11,216,841 max=494 min=55 avg=74
    52: data region reached 300000 times
/home/gskondras/cifar/cnn-cifar10_0/1.3-Parallel-pad/layers.c
  conv_forward  NVIDIA  devicenum=0
    time(us): 87,463,639
    72: compute region reached 150000 times
        72: kernel launched 150000 times
            grid: [1024]  block: [128]
             device time(us): total=87,463,639 max=1,092 min=181 avg=583
            elapsed time(us): total=88,947,445 max=1,104 min=189 avg=592
    72: data region reached 300000 times
```

### nsys

GPU Context
152.839 s


Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Category	Operation
84.4%	87.524 s	150000	583.496 μs	596.580 μs	181.153 μs	1.079 ms	323.180 μs	CUDA_KERNEL	conv_forward_72_gpu
10.0%	10.415 s	150000	69.430 μs	67.776 μs	8.800 μs	578.596 μs	18.329 μs	CUDA_KERNEL	pad_input_52_gpu
2.8%	2.878 s	2011510	1.430 μs	991 ns	480 ns	39.808 μs	1.656 μs	MEMORY_OPER	[CUDA memcpy Unified Device-to-Host]
2.8%	2.851 s	503319	5.665 μs	3.200 μs	1.727 μs	162.913 μs	6.131 μs	MEMORY_OPER	[CUDA memcpy Unified Host-to-Device]
0.0%	300.065 μs	3	100.021 μs	95.904 μs	28.768 μs	175.393 μs	73.399 μs	MEMORY_OPER	[CUDA memset]