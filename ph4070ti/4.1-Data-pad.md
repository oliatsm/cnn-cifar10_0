# Parallel Data, Padding
## Code
```c
52: void pad_input(float* restrict X, Conv_Layer* l) {
#pragma acc parallel loop 
  for ( int c = 0; c < l->in_depth; c++) {
    #pragma acc loop
    for (int j = 0; j < l->in_height; j++) {
      #pragma acc loop
      for (int i = 0; i < l->in_width; i++) {
        ...
}


70: void conv_forward(float* restrict X, Conv_Layer* l, float* restrict Y) {

  int in_size = l->in_width*l->in_height*l->in_depth;
    pad_input(X, l); //Create input with zero-padding
   // For each output feature map
#pragma acc parallel loop  
    for ( int m = 0; m < l->out_depth; m++) {
      #pragma acc loop
      for (int j = 0; j < l->out_height; j++) {
       #pragma acc loop
        for (int i = 0; i < l->out_width; i++) {
          int y_idx = i + (l->out_width * (j + m * l->out_height)); 
          // Calculate dot product of Weights*Input
          float sum = 0.0f;
        #pragma acc loop  reduction(+:sum) 
          for (int c = 0; c < l->in_depth; c++) {
            for (int f_j = 0; f_j < l->filter_width; f_j++) {
              for (int f_i = 0; f_i < l->filter_width; f_i++) {
                ...
```

## Minfo
```
$ make
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c main.c -o main.o
arr2txt:
    332, Zero trip check eliminated
arr2txt_2:
    354, Zero trip check eliminated
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c layers.c -o layers.o
make_conv_layer:
     42, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->weights_size])
         Generating enter data copyin(layer->in_padded[:layer->padded_size])
         Generating enter data create(layer->bias[:M])
free_conv:
     53, Generating exit data delete(l->bias[:l->out_depth],l->in_padded[:l->padded_size],l[:1],l->weights[:l->weights_size])
pad_input:
     60, Generating present(l[:],X[:])
         Generating NVIDIA GPU code
         62, #pragma acc loop gang /* blockIdx.x */
         64, #pragma acc loop seq
         66, #pragma acc loop vector(128) /* threadIdx.x */
     64, Loop is parallelizable
     66, Loop is parallelizable
conv_forward:
     82, Generating present(l[:])
         Generating copyout(Y[:l->out_size]) [if not already present]
         Generating copyin(X[:in_size]) [if not already present]
     83, Generating NVIDIA GPU code
         86, #pragma acc loop gang /* blockIdx.x */
         88, #pragma acc loop seq
         90, #pragma acc loop seq
         95, #pragma acc loop seq
         96, #pragma acc loop seq
         97, #pragma acc loop vector(128) /* threadIdx.x */
             Generating reduction(+:sum)
     88, Loop is parallelizable
     90, Loop is parallelizable
     95, Loop is parallelizable
     96, Loop is parallelizable
     97, Loop is parallelizable
    102, FMA (fused multiply-add) instruction(s) generated
pool_forward:
    185, Zero trip check eliminated
fc_forward:
    240, FMA (fused multiply-add) instruction(s) generated
load_conv:
    302, Generating update device(l->weights[:l->weights_size],l->bias[:l->out_depth])
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c malloc2D.c -o malloc2D.o
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -c timer.c -o timer.o
cpu_timer_stop:
     11, FMA (fused multiply-add) instruction(s) generated
nvc -acc -gpu=managed -Minfo=all -c11 -Wall -Wextra -march=native -o cnn-cifar10 main.o layers.o malloc2D.o timer.o
```

## Execution

```
$ ./cnn-cifar10 
Parallel (pad) Code
CNN for 50000 images
Loading input batch 1...
Loading input batch 2...
Loading input batch 3...
Loading input batch 4...
Loading input batch 5...
Load Data time:0.273425 seconds
Create Network time:0.000827 seconds
Load Network Parameters time:0.003356 seconds
Create Ouputs time:0.000276 seconds

Net Forward total time:125.529977 seconds
    Time for conv1: 51.667516 seconds
    Time for relu1: 11.344141 seconds
    Time for pool1: 1.608676 seconds
    Time for conv2: 33.970075 seconds
    Time for relu2: 7.658795 seconds
    Time for pool2: 1.720742 seconds
    Time for conv3: 12.384277 seconds
    Time for relu3: 4.327141 seconds
    Time for pool3: 0.699889 seconds
    Time for fc: 0.131167 seconds
    Time for softmax: 0.005138 seconds

  Conv: 98.021868 seconds
  ReLU: 23.330077 seconds
  Pool: 4.029307 seconds
  FC:   0.131167 seconds
  Softmax: 0.005138 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000272 seconds
Free memory time:0.016566 seconds
Total time:125.824700 seconds
```

## Profiling
### nv_time
```
$ ./cnn-cifar10 
Parallel (pad) Code
CNN for 50000 images
Loading input batch 1...
Loading input batch 2...
Loading input batch 3...
Loading input batch 4...
Loading input batch 5...
Load Data time:0.273079 seconds
Create Network time:0.000793 seconds
Load Network Parameters time:0.003390 seconds
Create Ouputs time:0.000270 seconds

Net Forward total time:128.137126 seconds
    Time for conv1: 52.475141 seconds
    Time for relu1: 11.407223 seconds
    Time for pool1: 1.609185 seconds
    Time for conv2: 34.773422 seconds
    Time for relu2: 7.725128 seconds
    Time for pool2: 1.723457 seconds
    Time for conv3: 13.188544 seconds
    Time for relu3: 4.384253 seconds
    Time for pool3: 0.700485 seconds
    Time for fc: 0.131216 seconds
    Time for softmax: 0.006350 seconds

  Conv: 100.437107 seconds
  ReLU: 23.516604 seconds
  Pool: 4.033127 seconds
  FC:   0.131216 seconds
  Softmax: 0.006350 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000272 seconds
Free memory time:0.016687 seconds
Total time:128.431617 seconds
END!

Accelerator Kernel Timing data
/home/gskondras/cifar/cnn-cifar10_0/1.3-Parallel-pad/layers.c
  make_conv_layer  NVIDIA  devicenum=0
    time(us): 0
    42: data region reached 9 times
/home/gskondras/cifar/cnn-cifar10_0/1.3-Parallel-pad/layers.c
  free_conv  NVIDIA  devicenum=0
    time(us): 0
    53: data region reached 9 times
/home/gskondras/cifar/cnn-cifar10_0/1.3-Parallel-pad/layers.c
  pad_input  NVIDIA  devicenum=0
    time(us): 9,077,248
    60: compute region reached 150000 times
        60: kernel launched 150000 times
            grid: [1024]  block: [128]
             device time(us): total=9,077,248 max=414 min=39 avg=60
            elapsed time(us): total=10,835,853 max=604 min=51 avg=72
    60: data region reached 300000 times
/home/gskondras/cifar/cnn-cifar10_0/1.3-Parallel-pad/layers.c
  conv_forward  NVIDIA  devicenum=0
    time(us): 87,325,549
    82: data region reached 300000 times
    83: compute region reached 150000 times
        83: kernel launched 150000 times
            grid: [1024]  block: [128]
             device time(us): total=87,325,549 max=1,065 min=181 avg=582
            elapsed time(us): total=88,815,401 max=1,077 min=189 avg=592
/home/gskondras/cifar/cnn-cifar10_0/1.3-Parallel-pad/layers.c
  load_conv  NVIDIA  devicenum=0
    time(us): 0
    302: update directive reached 3 times
```

### nsys
GPU Contexts
Run 170.500 s

Time	Total Time	Instances	Avg	Med	Min	Max	StdDev	Category	Operation
96.3%	143.866 s	150000	959.109 μs	1.053 ms	352.194 μs	2.161 ms	455.340 μs	CUDA_KERNEL	conv_forward_46_gpu
2.1%	3.081 s	809354	3.806 μs	3.199 μs	1.439 μs	163.329 μs	3.889 μs	MEMORY_OPER	[CUDA memcpy Unified Host-to-Device]
1.6%	2.421 s	1117044	2.167 μs	992 ns	575 ns	39.840 μs	2.366 μs	MEMORY_OPER	[CUDA memcpy Unified Device-to-Host]

