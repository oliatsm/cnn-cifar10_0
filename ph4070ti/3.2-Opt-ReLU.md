$ make
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native -c layers.c -o layers.o
make_conv_layer:
     42, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->weights_size])
         Generating enter data copyin(layer->in_padded[:layer->padded_size])
         Generating enter data create(layer->bias[:M])
free_conv:
     53, Generating exit data delete(l->bias[:l->out_depth],l->in_padded[:l->padded_size],l[:1],l->weights[:l->weights_size])
pad_input:
     60, Generating present(l[:],X[:])
         Generating implicit firstprivate(c)
         Generating NVIDIA GPU code
         62, #pragma acc loop gang, vector(128) collapse(3) /* blockIdx.x threadIdx.x */
         63,   /* blockIdx.x threadIdx.x collapsed */
         64,   /* blockIdx.x threadIdx.x collapsed */
conv_forward:
     79, Generating present(Y[:],l[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
         82, #pragma acc loop gang collapse(3) /* blockIdx.x */
         84,   /* blockIdx.x collapsed */
         86,   /* blockIdx.x collapsed */
         91, #pragma acc loop vector(32) collapse(2) /* threadIdx.x */
             Generating reduction(+:sum)
         92,   /* threadIdx.x collapsed */
         93, #pragma acc loop seq
     91, Loop is parallelizable
     92, Loop is parallelizable
     93, Loop is parallelizable
     98, FMA (fused multiply-add) instruction(s) generated
make_relu_layer:
    129, Generating enter data copyin(layer[:1])
relu_forward:
    134, Generating present(Y[:],l[:],X[:])
         Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        136, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */
free_relu:
    146, Generating exit data delete(l[:1])
make_pool_layer:
    173, Generating enter data copyin(layer[:1])
pool_forward:
    178, Generating present(l[:],Y[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
        181, #pragma acc loop gang collapse(3) /* blockIdx.x */
        182,   /* blockIdx.x collapsed */
        183,   /* blockIdx.x collapsed */
        188, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(max:max)
        189, #pragma acc loop seq
    188, Loop is parallelizable
    189, Loop is parallelizable
free_pool:
    208, Generating exit data delete(l[:1])
fc_forward:
    245, FMA (fused multiply-add) instruction(s) generated
load_conv:
    307, Generating update device(l->weights[:l->weights_size],l->bias[:l->out_depth])
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native -o cnn-cifar10 main.o layers.o malloc2D.o timer.o


Parallel (pad) Code
CNN for 50000 images
Load Data time:0.562043 seconds
Create Network time:0.031405 seconds
Load Network Parameters time:0.003633 seconds
Create Ouputs time:0.000172 seconds

Net Forward total time:5.938308 seconds
    Time for conv1: 1.419462 seconds
    Time for relu1: 0.320930 seconds
    Time for pool1: 0.497721 seconds
    Time for conv2: 1.126463 seconds
    Time for relu2: 0.322161 seconds
    Time for pool2: 0.369108 seconds
    Time for conv3: 0.797112 seconds
    Time for relu3: 0.322824 seconds
    Time for pool3: 0.340996 seconds
    Time for fc: 0.128982 seconds
    Time for softmax: 0.004981 seconds

  Conv: 3.343037 seconds
  ReLU: 0.965915 seconds
  Pool: 1.207825 seconds
  FC:   0.128982 seconds
  Softmax: 0.004981 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000261 seconds
Free memory time:0.030453 seconds
Total time:6.566275 seconds

---
```
$ make
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native -c layers.c -o layers.o
make_conv_layer:
     42, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->weights_size])
         Generating enter data copyin(layer->in_padded[:layer->padded_size])
         Generating enter data create(layer->bias[:M])
free_conv:
     53, Generating exit data delete(l->bias[:l->out_depth],l->in_padded[:l->padded_size],l[:1],l->weights[:l->weights_size])
pad_input:
     60, Generating present(l[:],X[:])
         Generating implicit firstprivate(c)
         Generating NVIDIA GPU code
         62, #pragma acc loop gang, vector(128) collapse(3) /* blockIdx.x threadIdx.x */
         63,   /* blockIdx.x threadIdx.x collapsed */
         64,   /* blockIdx.x threadIdx.x collapsed */
conv_forward:
     79, Generating present(Y[:],l[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
         82, #pragma acc loop gang collapse(3) /* blockIdx.x */
         84,   /* blockIdx.x collapsed */
         86,   /* blockIdx.x collapsed */
         91, #pragma acc loop vector(32) collapse(2) /* threadIdx.x */
             Generating reduction(+:sum)
         92,   /* threadIdx.x collapsed */
         93, #pragma acc loop seq
     91, Loop is parallelizable
     92, Loop is parallelizable
     93, Loop is parallelizable
     98, FMA (fused multiply-add) instruction(s) generated
make_relu_layer:
    129, Generating enter data copyin(layer[:1])
relu_forward:
    137, Generating present(Y[:],l[:],X[:])
    139, Loop is parallelizable
         Generating NVIDIA GPU code
        139, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.x threadIdx.y threadIdx.x */
free_relu:
    150, Generating exit data delete(l[:1])
make_pool_layer:
    177, Generating enter data copyin(layer[:1])
pool_forward:
    182, Generating present(l[:],Y[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
        185, #pragma acc loop gang collapse(3) /* blockIdx.x */
        186,   /* blockIdx.x collapsed */
        187,   /* blockIdx.x collapsed */
        192, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(max:max)
        193, #pragma acc loop seq
    192, Loop is parallelizable
    193, Loop is parallelizable
free_pool:
    212, Generating exit data delete(l[:1])
fc_forward:
    249, FMA (fused multiply-add) instruction(s) generated
load_conv:
    311, Generating update device(l->weights[:l->weights_size],l->bias[:l->out_depth])
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native -o cnn-cifar10 main.o layers.o malloc2D.o timer.o
```

```
$ ./cnn-cifar10
Parallel (pad) Code
CNN for 50000 images
Load Data time:0.576593 seconds
Create Network time:0.031376 seconds
Load Network Parameters time:0.003348 seconds
Create Ouputs time:0.000160 seconds

Net Forward total time:5.911564 seconds
    Time for conv1: 1.426215 seconds
    Time for relu1: 0.305037 seconds
    Time for pool1: 0.499613 seconds
    Time for conv2: 1.130551 seconds
    Time for relu2: 0.303072 seconds
    Time for pool2: 0.371001 seconds
    Time for conv3: 0.801812 seconds
    Time for relu3: 0.301589 seconds
    Time for pool3: 0.341803 seconds
    Time for fc: 0.129182 seconds
    Time for softmax: 0.005275 seconds

  Conv: 3.358578 seconds
  ReLU: 0.909699 seconds
  Pool: 1.212418 seconds
  FC:   0.129182 seconds
  Softmax: 0.005275 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000253 seconds
Free memory time:0.033470 seconds
Total time:6.556763 seconds
```