$ make
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native -c layers.c -o layers.o
make_conv_layer:
     42, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->weights_size])
         Generating enter data copyin(layer->in_padded[:layer->padded_size])
         Generating enter data create(layer->bias[:M])
free_conv:
     53, Generating exit data delete(l->bias[:l->out_depth],l->in_padded[:l->padded_size],l[:1],l->weights[:l->weights_size])
pad_input:
     60, Generating present(l[:],X[:])
         Generating implicit firstprivate(c)
         Generating NVIDIA GPU code
         62, #pragma acc loop gang, vector(128) collapse(3) /* blockIdx.x threadIdx.x */
         63,   /* blockIdx.x threadIdx.x collapsed */
         64,   /* blockIdx.x threadIdx.x collapsed */
conv_forward:
     79, Generating present(Y[:],l[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
         82, #pragma acc loop gang collapse(3) /* blockIdx.x */
         84,   /* blockIdx.x collapsed */
         86,   /* blockIdx.x collapsed */
         91, #pragma acc loop vector(32) collapse(2) /* threadIdx.x */
             Generating reduction(+:sum)
         92,   /* threadIdx.x collapsed */
         93, #pragma acc loop seq
     91, Loop is parallelizable
     92, Loop is parallelizable
     93, Loop is parallelizable
     98, FMA (fused multiply-add) instruction(s) generated
make_relu_layer:
    129, Generating enter data copyin(layer[:1])
relu_forward:
    134, Generating present(Y[:],l[:],X[:])
         Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        136, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */
free_relu:
    146, Generating exit data delete(l[:1])
make_pool_layer:
    173, Generating enter data copyin(layer[:1])
pool_forward:
    178, Generating present(l[:],Y[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
        181, #pragma acc loop gang collapse(3) /* blockIdx.x */
        182,   /* blockIdx.x collapsed */
        183,   /* blockIdx.x collapsed */
        188, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(max:max)
        189, #pragma acc loop seq
    188, Loop is parallelizable
    189, Loop is parallelizable
free_pool:
    208, Generating exit data delete(l[:1])
make_fc_layer:
    236, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->out_depth*layer->in_neurons],layer->bias[:layer->out_depth])
fc_forward:
    241, Generating present(l[:],Y[:],X[:])
         Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        244, #pragma acc loop gang /* blockIdx.x */
        248, #pragma acc loop vector(128) /* threadIdx.x */
             Generating reduction(+:sum)
    248, Loop is parallelizable
    250, FMA (fused multiply-add) instruction(s) generated
free_fc:
    263, Generating exit data delete(l->bias[:l->out_depth],l[:1],l->weights[:l->out_depth*l->in_neurons])
load_conv:
    312, Generating update device(l->weights[:l->weights_size],l->bias[:l->out_depth])
load_fc:
    360, Generating update device(l->weights[:l->out_depth*l->in_neurons],l->bias[:l->out_depth])
make_softmax_layer:
    386, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->likelihoods[:layer->out_depth])
softmax_forward:
    396, Generating copyin(max,total) [if not already present]
         Generating present(l[:],Y[:],X[:])
    401, Loop is parallelizable
         Generating NVIDIA GPU code
        401, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(max:max)
    409, Loop is parallelizable
         Generating NVIDIA GPU code
        409, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(+:total)
    417, Loop is parallelizable
         Generating NVIDIA GPU code
        417, #pragma acc loop vector(32) /* threadIdx.x */
free_softmax:
    429, Generating exit data delete(l[:1],l->likelihoods[:l->out_depth])

$ ./cnn-cifar10 
Parallel (pad) Code
CNN for 50000 images
Load Data time:0.565613 seconds
Create Network time:0.031395 seconds
Load Network Parameters time:0.003476 seconds
Create Ouputs time:0.000158 seconds

Net Forward total time:6.719522 seconds
    Time for conv1: 1.420482 seconds
    Time for relu1: 0.325402 seconds
    Time for pool1: 0.499475 seconds
    Time for conv2: 1.129538 seconds
    Time for relu2: 0.325017 seconds
    Time for pool2: 0.370679 seconds
    Time for conv3: 0.800903 seconds
    Time for relu3: 0.325493 seconds
    Time for pool3: 0.341337 seconds
    Time for fc: 0.361900 seconds
    Time for softmax: 0.802657 seconds

  Conv: 3.350923 seconds
  ReLU: 0.975912 seconds
  Pool: 1.211491 seconds
  FC:   0.361900 seconds
  Softmax: 0.802657 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000312 seconds
Free memory time:0.030343 seconds
Total time:7.350820 seconds

Στην συνάρτηση Softmax καλύτερη απόδοση έχει το kernels.

Ο υπολογισμός της συνάρτησης fc_forward και softmax_forward στη κάρτα γραφικών δεν βελτιώνει την ταχύτητα του κώδικα. Καθώς το συγκεκριμένο σύνολο δεδομένων έχει μόνο 10 κατηγορίες, οι πράξεις στα δύο τελευταία επίπεδα δεν συνφαίρει να γίνουν στην κάρτα γραφικών. Αντίθετα, προσθέτει καθυστέριση.

---

```
$ make
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native -c layers.c -o layers.o
make_conv_layer:
     42, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->weights_size])
         Generating enter data copyin(layer->in_padded[:layer->padded_size])
         Generating enter data create(layer->bias[:M])
free_conv:
     53, Generating exit data delete(l->bias[:l->out_depth],l->in_padded[:l->padded_size],l[:1],l->weights[:l->weights_size])
pad_input:
     60, Generating present(l[:],X[:])
         Generating implicit firstprivate(c)
         Generating NVIDIA GPU code
         62, #pragma acc loop gang, vector(128) collapse(3) /* blockIdx.x threadIdx.x */
         63,   /* blockIdx.x threadIdx.x collapsed */
         64,   /* blockIdx.x threadIdx.x collapsed */
conv_forward:
     79, Generating present(Y[:],l[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
         82, #pragma acc loop gang collapse(3) /* blockIdx.x */
         84,   /* blockIdx.x collapsed */
         86,   /* blockIdx.x collapsed */
         91, #pragma acc loop vector(32) collapse(2) /* threadIdx.x */
             Generating reduction(+:sum)
         92,   /* threadIdx.x collapsed */
         93, #pragma acc loop seq
     91, Loop is parallelizable
     92, Loop is parallelizable
     93, Loop is parallelizable
     98, FMA (fused multiply-add) instruction(s) generated
make_relu_layer:
    129, Generating enter data copyin(layer[:1])
relu_forward:
    137, Generating present(Y[:],l[:],X[:])
    139, Loop is parallelizable
         Generating NVIDIA GPU code
        139, #pragma acc loop gang, worker(4), vector(32) /* blockIdx.x threadIdx.y threadIdx.x */
free_relu:
    150, Generating exit data delete(l[:1])
make_pool_layer:
    177, Generating enter data copyin(layer[:1])
pool_forward:
    182, Generating present(l[:],Y[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
        185, #pragma acc loop gang collapse(3) /* blockIdx.x */
        186,   /* blockIdx.x collapsed */
        187,   /* blockIdx.x collapsed */
        192, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(max:max)
        193, #pragma acc loop seq
    192, Loop is parallelizable
    193, Loop is parallelizable
free_pool:
    212, Generating exit data delete(l[:1])
make_fc_layer:
    240, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->out_depth*layer->in_neurons],layer->bias[:layer->out_depth])
fc_forward:
    249, Generating present(Y[:],l[:],X[:])
    251, Loop is parallelizable
         Generating NVIDIA GPU code
        251, #pragma acc loop gang(10) /* blockIdx.x */
        255, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(+:sum)
    255, Loop is parallelizable
    257, FMA (fused multiply-add) instruction(s) generated
free_fc:
    271, Generating exit data delete(l->bias[:l->out_depth],l[:1],l->weights[:l->out_depth*l->in_neurons])
load_conv:
    320, Generating update device(l->weights[:l->weights_size],l->bias[:l->out_depth])
load_fc:
    368, Generating update device(l->weights[:l->out_depth*l->in_neurons],l->bias[:l->out_depth])
make_softmax_layer:
    394, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->likelihoods[:layer->out_depth])
softmax_forward:
    404, Generating copyin(max,total) [if not already present]
         Generating present(l[:],Y[:],X[:])
    409, Loop is parallelizable
         Generating NVIDIA GPU code
        409, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(max:max)
    417, Loop is parallelizable
         Generating NVIDIA GPU code
        417, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(+:total)
    425, Loop is parallelizable
         Generating NVIDIA GPU code
        425, #pragma acc loop vector(32) /* threadIdx.x */
free_softmax:
    437, Generating exit data delete(l[:1],l->likelihoods[:l->out_depth])
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native -o cnn-cifar10 main.o layers.o malloc2D.o timer.o

$ ./cnn-cifar10 
Parallel (pad) Code
CNN for 50000 images
Load Data time:0.568991 seconds
Create Network time:0.031377 seconds
Load Network Parameters time:0.003362 seconds
Create Ouputs time:0.000160 seconds

Net Forward total time:6.625489 seconds
    Time for conv1: 1.421303 seconds
    Time for relu1: 0.303744 seconds
    Time for pool1: 0.497754 seconds
    Time for conv2: 1.129416 seconds
    Time for relu2: 0.302210 seconds
    Time for pool2: 0.368636 seconds
    Time for conv3: 0.800972 seconds
    Time for relu3: 0.301027 seconds
    Time for pool3: 0.340050 seconds
    Time for fc: 0.348934 seconds
    Time for softmax: 0.794975 seconds

  Conv: 3.351690 seconds
  ReLU: 0.906980 seconds
  Pool: 1.206439 seconds
  FC:   0.348934 seconds
  Softmax: 0.794975 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000316 seconds
Free memory time:0.033467 seconds
Total time:7.263163 seconds
```