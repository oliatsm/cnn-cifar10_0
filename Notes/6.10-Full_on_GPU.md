# Full forward on GPU


```c
File: layers.c
...
060: void pad_input(float* restrict X, Conv_Layer* l) {
061: 
062: #pragma acc parallel loop present(l,X)  gang vector_length(32)
063:   for (int c = 0; c < l->in_depth; c++) {
064:   #pragma acc loop 
065:     for (int j = 0; j < l->in_height; j++) {
066:     #pragma acc loop vector
067:       for (int i = 0; i < l->in_width; i++) {
068:         int padded_idx = (j + l->padding) * l->padded_width + (i + l->padding) + c * l->padded_height * l->padded_width;
069:         int in_idx = j * l->in_width + i + c * l->in_height * l->in_width;
070:         l->in_padded[padded_idx] = X[in_idx];
071:       }
072:     }
073:   }
074: }
...
079: void conv_forward(float* restrict X, Conv_Layer* l, float* restrict Y) {
080: 
081:   pad_input(X, l); //Create input with zero-padding
082:   // For each output feature map
083: #pragma acc parallel loop present(l,Y) gang worker collapse(3) vector_length(32)
084:   for (int m = 0; m < l->out_depth; m++) {
085:     for (int j = 0; j < l->out_height; j++) {
086:       for (int i = 0; i < l->out_width; i++) {
087:         int y_idx = i + (l->out_width * (j + m * l->out_height)); // Output index
088:         // Calculate dot product of Weights*Input
089:         float sum = 0.0f;
090:       #pragma acc loop reduction(+:sum) vector collapse(2) 
091:         for (int c = 0; c < l->in_depth; c++) {
092:           for (int f_j = 0; f_j < l->filter_width; f_j++) {
093:             for (int f_i = 0; f_i < l->filter_width; f_i++) {
094:               int f_idx = f_i + (f_j * l->filter_width) + (c + m * l->in_depth) * (l->filter_width * l->filter_width); // Filter Index
095:               int x_j = j * l->stride + f_j; // Input height index, increased by stride
096:               int x_i = i * l->stride + f_i; // Input width index, increased by stride
097:               int x_idx = c * l->padded_height * l->padded_width + x_j * l->padded_width + x_i; // Input index
098:               sum += l->weights[f_idx] * l->in_padded[x_idx];
099:             } // for f_i
100:           } // for f_j
101:         } // for c
102:         sum += l->bias[m]; // Add bias
103:         Y[y_idx] = sum; // Save output result
104:       } // for i
105:     } // for j
106:   } // for m
107: }
108: 
...
135: void relu_forward(float* restrict X, ReLU_Layer* l, float* restrict Y) {
136: 
137: #pragma acc parallel loop present(l,X,Y) gang vector vector_length(32)
138:   for (int i = 0; i < l->out_size; i++) {
139:     Y[i] = (X[i] < 0.0f) ? 0.0f : X[i];
140:   }
141: }
142: 
...
181: void pool_forward(float* restrict X, Pool_Layer* l, float* restrict Y) {
182:   // For each output feature map
183: #pragma acc parallel loop present(X,l,Y) gang worker collapse(3) vector_length(32)
184:   for (int m = 0; m < l->out_depth; m++) {
185:     for (int j = 0; j < l->out_height; j++) {
186:       for (int i = 0; i < l->out_width; i++) {
187:         int y_idx = i + l->out_width * (j + m * l->out_height); // Output index
188:         // Find Max in pooling filter
189:         float max = -INFINITY;
190:       #pragma acc loop reduction(max:max) vector collapse(2)
191:         for (int p_j = 0; p_j < l->pool_width; p_j++) {
192:           for (int p_i = 0; p_i < l->pool_width; p_i++) {
193:             int x_j = j * l->stride + p_j; // Input height index, increased by stride
194:             int x_i = i * l->stride + p_i; // Input width index, increased by stride
195:             int x_idx = x_i + (x_j + m * l->in_height) * l->in_width; // Input index
196:             if (X[x_idx] > max) {
197:               max = X[x_idx];
198:             } // if max
199:           } // for p_i
200:         } // for p_j
201:         Y[y_idx] = max;
202:       } // for i
203:     } // for j
204:   } // for m
205: }
206: 
...
247: void fc_forward(float* restrict X, FC_Layer* l, float* restrict Y) {
248: 
249:   // For every output neuron
250: #pragma acc parallel loop present(l,X,Y) gang worker vector_length(32)
251:   for (int i = 0; i < l->out_depth; i++) {
252:     // Calculate dot product of input and weights
253:     float sum = 0.0f;
254:   #pragma loop reduction(+:sum) vector
255:     for (int j = 0; j < l->in_neurons; j++) {
256:       int w_idx = j + i * l->in_neurons; // Weight index
257:       sum += X[j] * l->weights[w_idx];
258:     }
259:     sum += l->bias[i]; // add bias
260:     Y[i] = sum;
261:   }
262: }
...
...
405: void softmax_forward(float* restrict X, Softmax_Layer* l, float* restrict Y) {
406: 
407:   float max = -INFINITY;
408:   float total = 0.0f;
409: 
410: #pragma acc data present(X,l,Y) create(max,total) cache(X[0:10])
411:   {
412:     // Compute max activation
413:   #pragma acc parallel loop reduction(max:max) vector vector_length(32)
414:     for (int i = 0; i < l->out_depth; i++) {
415:       if (X[i] > max) {
416:         max = X[i];
417:       }
418:     }
419: 
420:     // Compute exponentials and total
421:   #pragma acc parallel loop reduction(+:total) vector vector_length(32)
422:     for (int i = 0; i < l->out_depth; i++) {
423:       float e = exp(X[i] - max);
424:       total += e;
425:       l->likelihoods[i] = e;
426:     }
427: 
428:     // Normalize and output to sum to one
429:   #pragma acc parallel loop vector vector_length(32)
430:     for (int i = 0; i < l->out_depth; i++) {
431:       Y[i] = l->likelihoods[i] / total;
432:     }
433:   }
434: }
...
447: 
```

best opts:
 **Use vector length 32, because GPU warp size is 32.**

```
$ make
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native  -c layers.c -o layers.o
make_conv_layer:
     42, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->weights_size])
         Generating enter data copyin(layer->in_padded[:layer->padded_size])
         Generating enter data create(layer->bias[:M])
free_conv:
     53, Generating exit data delete(l->bias[:l->out_depth],l->in_padded[:l->padded_size],l[:1],l->weights[:l->weights_size])
pad_input:
     60, Generating present(X[:],l[:])
         Generating implicit firstprivate(c)
         Generating NVIDIA GPU code
         63, #pragma acc loop gang /* blockIdx.x */
         65, #pragma acc loop seq
         67, #pragma acc loop vector(32) /* threadIdx.x */
     65, Loop is parallelizable
     67, Loop is parallelizable
conv_forward:
     81, Generating present(l[:],Y[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
         84, #pragma acc loop gang, worker(4) collapse(3) /* blockIdx.x threadIdx.y */
         85,   /* blockIdx.x threadIdx.y collapsed */
         86,   /* blockIdx.x threadIdx.y collapsed */
         91, #pragma acc loop vector(32) collapse(2) /* threadIdx.x */
             Generating reduction(+:sum)
         92,   /* threadIdx.x collapsed */
         93, #pragma acc loop seq
        100, Vector barrier inserted for vector loop reduction
             Vector barrier inserted due to potential dependence out of a vector loop
     91, Loop is parallelizable
     92, Loop is parallelizable
     93, Loop is parallelizable
     98, FMA (fused multiply-add) instruction(s) generated
make_relu_layer:
    130, Generating enter data copyin(layer[:1])
relu_forward:
    135, Generating present(Y[:],l[:],X[:])
         Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        138, #pragma acc loop gang, vector(32) /* blockIdx.x threadIdx.x */
free_relu:
    148, Generating exit data delete(l[:1])
make_pool_layer:
    176, Generating enter data copyin(layer[:1])
pool_forward:
    181, Generating present(l[:],Y[:],X[:])
         Generating implicit firstprivate(m)
         Generating NVIDIA GPU code
        184, #pragma acc loop gang, worker(4) collapse(3) /* blockIdx.x threadIdx.y */
        185,   /* blockIdx.x threadIdx.y collapsed */
        186,   /* blockIdx.x threadIdx.y collapsed */
        191, #pragma acc loop vector(32) collapse(2) /* threadIdx.x */
             Generating reduction(max:max)
        192,   /* threadIdx.x collapsed */
        199, Vector barrier inserted for vector loop reduction
             Vector barrier inserted due to potential dependence out of a vector loop
    191, Loop is parallelizable
    192, Loop is parallelizable
free_pool:
    212, Generating exit data delete(l[:1])
make_fc_layer:
    242, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->weights[:layer->out_depth*layer->in_neurons],layer->bias[:layer->out_depth])
fc_forward:
    247, Generating present(l[:],Y[:],X[:])
         Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        251, #pragma acc loop gang, worker(4) /* blockIdx.x threadIdx.y */
        255, #pragma acc loop vector(32) /* threadIdx.x */
             Generating implicit reduction(+:sum)
        257, Vector barrier inserted for vector loop reduction
    255, Loop is parallelizable
    257, FMA (fused multiply-add) instruction(s) generated
free_fc:
    273, Generating exit data delete(l->bias[:l->out_depth],l[:1],l->weights[:l->out_depth*l->in_neurons])
load_conv:
    322, Generating update device(l->weights[:l->weights_size],l->bias[:l->out_depth])
load_fc:
    373, Generating update device(l->weights[:l->out_depth*l->in_neurons],l->bias[:l->out_depth])
make_softmax_layer:
    400, Generating enter data copyin(layer[:1])
         Generating enter data create(layer->likelihoods[:layer->out_depth])
softmax_forward:
    411, Generating present(l[:],Y[:],X[:])
         Generating create(total,max) [if not already present]
         Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        414, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(max:max)
    414, Loop is parallelizable
    418, Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        422, #pragma acc loop vector(32) /* threadIdx.x */
             Generating reduction(+:total)
    422, Loop is parallelizable
    426, Generating implicit firstprivate(i)
         Generating NVIDIA GPU code
        430, #pragma acc loop vector(32) /* threadIdx.x */
    430, Loop is parallelizable
free_softmax:
    444, Generating exit data delete(l[:1],l->likelihoods[:l->out_depth])
nvc -acc -Minfo=all -c11 -Wall -Wextra -march=native  -o cnn-cifar10 main.o layers.o malloc2D.o timer.o
olia@krylov100:~/Diplomatiki/cnn-cifar10_0/serial2parallel-4$ ./cnn-cifar10 
Parallel Code
CNN for 50000 images
Loading input batch 1...
Loading input batch 2...
Loading input batch 3...
Loading input batch 4...
Loading input batch 5...
Load Data time:0.678608 seconds
Create Network time:0.169604 seconds
Load Network Parameters time:0.007930 seconds
Create Ouputs time:0.000318 seconds

Net Forward total time:11.058935 seconds
    Time for conv1: 2.078987 seconds
    Time for relu1: 0.680793 seconds
    Time for pool1: 0.679397 seconds
    Time for conv2: 1.854469 seconds
    Time for relu2: 0.679914 seconds
    Time for pool2: 0.567739 seconds
    Time for conv3: 1.384512 seconds
    Time for relu3: 0.683641 seconds
    Time for pool3: 0.524825 seconds
    Time for fc: 0.566316 seconds
    Time for softmax: 1.339671 seconds

  Conv: 5.317968 seconds
  ReLU: 2.044347 seconds
  Pool: 1.771961 seconds
  FC:   0.566316 seconds
  Softmax: 1.339671 seconds

Net Accuracy: 78.84 % 
Net Accuracy time:0.000804 seconds
Free memory time:0.046795 seconds
Total time:11.962994 seconds
END!
```